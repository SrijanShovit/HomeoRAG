{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc495327",
   "metadata": {},
   "source": [
    "### Get File Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41d05ec2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size in MB: 0.783\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "file_path = \"JudgeFlow.pdf\"\n",
    "\n",
    "path = Path(file_path)\n",
    "size_bytes = path.stat().st_size\n",
    "\n",
    "size_kb = size_bytes / 1024\n",
    "size_mb = size_bytes / (1024 ** 2)\n",
    "\n",
    "print(f\"Size in MB: {size_mb:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a52b0a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9e559d5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">This is white text\n",
       "</pre>\n"
      ],
      "text/plain": [
       "This is white text\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Still white</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Still white\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from rich.console import Console\n",
    "from rich.theme import Theme\n",
    "\n",
    "white_theme = Theme({\n",
    "    \"default\": \"white\",\n",
    "})\n",
    "\n",
    "console = Console(\n",
    "    width=140,\n",
    "    theme=white_theme,\n",
    "    color_system=None  # disables ANSI color negotiation\n",
    ")\n",
    "\n",
    "console.print(\"This is white text\")\n",
    "console.print(\"[bold]Still white[/bold]\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d440066",
   "metadata": {},
   "source": [
    "### Read and Load File Contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "89234010",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pypdf import PdfReader\n",
    "\n",
    "def read_pdf(path: str) -> str:\n",
    "    reader = PdfReader(path)\n",
    "    text = []\n",
    "    for page in reader.pages:\n",
    "        page_text = page.extract_text()\n",
    "        if page_text:\n",
    "            text.append(page_text)\n",
    "    return \"\\n\".join(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "29d2deae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JUDGEFLOW: AGENTICWORKFLOWOPTIMIZATION\n",
      "VIABLOCKJUDGE\n",
      "Zihan Ma∗,1 Zhikai Zhao∗,1 Chuanbo Hua1 Federico Berto1,2 Jinkyoo Park1,3\n",
      "1KAIST 2Radical Numerics 3Omelet\n",
      "ABSTRACT\n",
      "Optimizing LLM-based agentic workflows is challenging for scaling AI capabilities.\n",
      "Current methods rely on coarse, end-to-end evaluation signals and lack fine-grained\n",
      "signals on where to refine, often resulting in inefficient or low-impact modifica-\n",
      "tions. To address these limitations, we propose JUDGEFLOW, an Evaluation-\n",
      "Judge-Optimization-Update pipeline. We incorporate reusable, configurable logic\n",
      "blocks into agentic workflows to capture fundamental forms of logic. On top\n",
      "of this abstraction, we design a dedicated Judge module that inspects execution\n",
      "traces – particularly failed runs – and assigns rank-based responsibility scores to\n",
      "problematic blocks. These fine-grained diagnostic signals are then leveraged by\n",
      "an LLM-based optimizer, which focuses modifications on the most problematic\n",
      "block in the workflow. Our approach improves sample efficiency, enhances inter-\n",
      "pretability through block-level diagnostics, and provides a scalable foundation for\n",
      "automating increasingly complex agentic workflows. We evaluate JUDGEFLOW\n",
      "on mathematical reasoning and code generation benchmarks, where JUDGEFLOW\n",
      "achieves superior performance and efficiency compared to existing methods. The\n",
      "source code is publicly available athttps://github.com/ma-zihan/JudgeFlow.\n",
      "1 INTRODUCTION\n",
      "Large language models (LLMs) (Brown et al., 2020) have achieved remarkable success across a\n",
      "wide range of domains. Moving beyond the scope of foundation models (Bommasani et al., 2022),\n",
      "by integrating LLMs into intelligent agent architectures, the emerging foundation agents (Liu et al.,\n",
      "2025a) have attracted more attention. Starting from early work on prompt engineering, such as\n",
      "reasoning-enhanced methods (Wei et al., 2023; Wang et al., 2023b; Yao et al., 2023a), to more recent\n",
      "developments in multi-agent system approaches (Du et al., 2023; Li et al., 2023; Hong et al., 2024),\n",
      "these handcrafted strategies have achieved strong performance across a range of tasks, including\n",
      "mathematical reasoning (Cobbe et al., 2021), code generation (Austin et al., 2021), and question\n",
      "answering (Yang et al., 2018).\n",
      "However, these agentic systems still depend heavily on manual design, making workflow construction\n",
      "complex, costly, and inflexible. AutoML (Hutter et al., 2019) has shown that automating handcrafted\n",
      "and labor-intensive processes in machine learning can substantially reduce human effort and accelerate\n",
      "the development of high-performance models. Inspired by this success, recent efforts aim to automate\n",
      "the design and optimization of LLM-based agentic workflows (Lee et al., 2025). While these agentic\n",
      "systems still rely on LLMs as core execution engines, optimizing the LLMs themselves through\n",
      "pretraining or fine-tuning (Rafailov et al., 2023) often demands substantial computational resources\n",
      "and massive-scale data, making such approaches expensive in many settings (Kaplan et al., 2020).\n",
      "Instead, keeping the underlying model parameters fixed, and focusing on optimizing the systems\n",
      "structure and behavior leads to a more tractable and efficient optimization.\n",
      "Automation efforts in agentic systems initially focused on prompt optimization, exemplified by\n",
      "Textual Gradients which leverage LLM feedback for end-to-end optimization (Pryzant et al., 2023;\n",
      "Yuksekgonul et al., 2024; Wang et al., 2024b; Yin & Wang, 2025). Current efforts are expanding to\n",
      "optimize architecture and execution flow of entire agentic systems. Agentic workflow can be modeled\n",
      "as neural network (Liu et al., 2024; Ma et al., 2025), graph (Zhuge et al., 2024a; Zhang et al., 2025a),\n",
      "∗Equal contributions\n",
      "1\n",
      "arXiv:2601.07477v1  [cs.AI]  12 Jan 2026\n",
      "and code (Hu et al., 2025; Zhang et al., 2025b; Zheng et al., 2025), each offering different levels of\n",
      "representational capacity, interpretability, and optimization difficulty. For instance, Directed Acyclic\n",
      "Graphs (DAGs)-represented workflows facilitate tractable optimization but constrain the ability to\n",
      "represent complex structures such as loops or conditional branching. In contrast, code-represented\n",
      "workflows provide comprehensive expressivity in defining intricate logic and control flow, but error\n",
      "attribution within code execution is difficult, and optimization often has to rely solely on end-to-end\n",
      "evaluation signals rather than fine-grained intermediate feedback. Building on code-represented\n",
      "workflows, Zhang et al. (2025b) introduce operators as modular units that encapsulate common\n",
      "agentic actions and propose a Monte Carlo Tree Search (MCTS) framework that employs LLMs to\n",
      "iteratively optimize workflow structures using past experience. However, the expansion phase in\n",
      "MCTS and the subsequent evaluation of candidate workflows can be expensive, and the effectiveness\n",
      "of the optimization process is constrained by the granularity of guidance available for modifications.\n",
      "In the absence of sufficiently fine-grained diagnostic information to precisely identify which specific\n",
      "part within the complex workflow requires modification, the search may explore ineffective or\n",
      "low-impact alterations. Furthermore, complex code structural interactions—such as conditional\n",
      "constructs where only one branch of an if-else statement is executed along a trajectory—leave\n",
      "certain components without informative signals, thereby hindering fine-grained analysis.\n",
      "To address these challenges, we introduce JUDGEFLOW, an Evaluation-Judge-Optimization-Update\n",
      "pipeline. First, we incorporate reusable and configurable logic blocks into agentic workflows. These\n",
      "blocks capture three fundamental forms of logic: sequential, loop, and conditional, which are able to\n",
      "broadly represent code-based workflows. Compared with operators, which abstract specific agentic\n",
      "operations or functionalities (Zhang et al., 2025b), logic blocks serve as higher-level structural\n",
      "abstractions. By introducing logic blocks that abstract such common control structures, we retain the\n",
      "structural diversity of code-represented workflows while providing an intermediate level of abstraction\n",
      "between operators and workflows. This additional layer facilitates interpretability and exposes more\n",
      "meaningful diagnostic information for subsequent optimization.\n",
      "Second, we incorporate a dedicated Judge module\n",
      "that analyzes the execution trace, with particular\n",
      "emphasis on failed runs. We hypothesize that op-\n",
      "timizers should receive both evaluation and opti-\n",
      "mization signals. For each unsuccessful execution,\n",
      "the Judge attempts to identify the most problematic\n",
      "block within the workflow as illustrated in Fig. 1.\n",
      "To further improve the precision of diagnosis, we\n",
      "adopt a rank-based approach at the block level.\n",
      "The resulting targeted diagnostic signals are prop-\n",
      "agated to the subsequent optimization stage, en-\n",
      "abling more focused and efficient refinement of\n",
      "weak blocks. In this way, optimization efforts\n",
      "can be concentrated on repairing underperform-\n",
      "ing components, resulting in more effective and\n",
      "reliable improvements in overall workflow perfor-\n",
      "mance. Besides relying solely on end-to-end eval-\n",
      "uation signals, our approach leverages block-level\n",
      "diagnostic information, enabling the optimizer to\n",
      "focus on the most problematic components.\n",
      "Question Workflow Answer\n",
      "LLM\n",
      "Optimizer\n",
      "Which part should I focus on...?\n",
      "Previous Works\n",
      "Question Workflow Answer\n",
      "I know where the problem is!\n",
      "JudgeFlow\n",
      "Judge\n",
      "LLM\n",
      "Optimizer\n",
      "Figure 1: Block-level judging guides agentic\n",
      "workflow optimization by identifying the most\n",
      "problematic block in failed executions.\n",
      "In summary, our contributions are as follows:\n",
      "• We propose a novel Evaluation-Judge-Optimization-Update pipeline named JUDGEFLOW;\n",
      "• We introduce reusable and configurable logic blocks as higher-level structural units, which\n",
      "balance the expressivity of code-based workflows with tractable optimization, while sup-\n",
      "porting interpretability and intermediate execution tracing;\n",
      "• We design a Judge module that analyzes execution traces, especially failed runs, and\n",
      "assigns rank-based responsibility scores to problematic blocks, enabling fine-grained error\n",
      "localization and targeted refinement for subsequent optimization.\n",
      "• We evaluate JUDGEFLOWon mathematical reasoning and code generation benchmarks,\n",
      "showing that it outperforms existing methods.\n",
      "2\n",
      "2 RELATEDWORK\n",
      "LLM-based (Multi-)Agent SystemsIn recent years, LLM-based (multi-)agent systems have\n",
      "achieved notable successes (Wang et al., 2024a; Huang et al., 2024; Tran et al., 2025) across\n",
      "planning (Huang et al., 2024), reasoning (Putta et al., 2024), and human–AI coordination (Zou\n",
      "et al., 2025). At the single-agent level, foundational works have enabled agents to reason and\n",
      "act by interleaving thought and action (Yao et al., 2023b), to enhance complex problem-solving\n",
      "through structured exploration of thoughts (Yao et al., 2023a), and to interact effectively with external\n",
      "tools and APIs (Wu et al., 2024). At the multi-agent level, frameworks such as CAMEL (Li et al.,\n",
      "2023), AutoGen (Wu et al., 2023), and MetaGPT (Hong et al., 2024) have facilitated sophisticated\n",
      "collaboration on complex tasks like software development, demonstrating strong performance across\n",
      "diverse domains. Despite these advances, existing systems remain constrained by a reliance on\n",
      "handcrafted prompts and rigid communication topologies, which limit adaptability as task complexity\n",
      "scales. This has spurred a shift toward automated agentic systems capable of optimizing their own\n",
      "architectures and behaviors.\n",
      "Agentic Systems AutomationEarly automation efforts in agentic systems primarily focused on\n",
      "prompt optimization (Pryzant et al., 2023; Ramnath et al., 2025; Li et al., 2025), with approaches\n",
      "such as LLMs-as-optimizers (Yang et al., 2024), self-referential evolution (Fernando et al., 2023),\n",
      "textual gradients (Yuksekgonul et al., 2024), and self-supervised optimization (Xiang et al., 2025).\n",
      "More recent research has expanded beyond prompt-level tuning toward optimizing the architectures\n",
      "and execution flows of entire agentic systems. For example, Liu et al. (2024) explores dynamic\n",
      "communication structures for adaptive collaboration, while Zhuge et al. (2024a) models agents\n",
      "as computational graphs to refine both prompts and inter-agent orchestration. Shang et al. (2024)\n",
      "proposes a novel modular design automatically searching for high-performance agent structures.\n",
      "Zhou et al. (2024) investigates agents capable of self-optimization using symbolic optimizers. Hu et al.\n",
      "(2025) introduces a meta agent that automatically discovers novel, high-performing, and generalizable\n",
      "agentic system designs. Yin et al. (2025) introduces a self-referential framework that enables agents\n",
      "to recursively improve themselves. Zhang et al. (2025b) employs LLMs as optimizers with a Monte\n",
      "Carlo Tree Search (MCTS) variant to discover effective workflows. Zhang et al. (2025a) automatically\n",
      "evolve agentic supernet systems leading to query-specific workflows. Su et al. (2025) leverages\n",
      "debate and reflexion to collaboratively refine workflows while reducing search redundancy. Zheng\n",
      "et al. (2025) introduces safety-constrained evolutionary programming in a declarative graph space,\n",
      "ensuring structural validity and robustness. While these efforts mark significant progress, most\n",
      "existing approaches still focus on end-to-end or global architectural optimization, often leading to\n",
      "inefficient search and a lack of fine-grained diagnostic feedback, which limits both scalability and\n",
      "interpretability as task complexity grows.\n",
      "LLM as a JudgeThe LLM-as-a-judge paradigm leverages large language models to automate the\n",
      "evaluation of generated content, addressing the scalability limitations of human assessment (Gu et al.,\n",
      "2025). This approach has been widely adopted for assessing complex outputs based on predefined\n",
      "criteria like correctness, relevance, or rule compliance (Li et al., 2024). However, the effectiveness of\n",
      "the LLM-as-a-Judge framework may be limited by inherent biases in LLMs (Wang et al., 2023a). To\n",
      "mitigate these issues, various methods have been proposed. Liu et al. (2025b) propose a ranking-based\n",
      "alignment method that significantly improves the judging performance of LLMs. In addition, (Zhuge\n",
      "et al., 2024b) proposed the framework to use agentic systems to evaluate agentic systems. In a related\n",
      "application, (Zhang et al., 2025c) attempts to automate the failure attribution for LLM multi-agent\n",
      "systems. Their findings reveal that providing stronger ground-truth signals can substantially improve\n",
      "attribution quality, and aggregated analysis across multiple failures can uncover reliable error patterns.\n",
      "3 METHODOLOGY\n",
      "3.1 PROBLEMFORMULATION\n",
      "Our framework models an agentic workflow by hierarchically composing basic agentic actions\n",
      "(Operators) into structured logical units (Blocks) as follows.\n",
      "3\n",
      "A configured operator O(D) is the basic unit of agentic action, where O represents a categorical\n",
      "label for its core function like generate or self_refine (details in Appendix A), and D is the\n",
      "operator configuration, which includes the LLM backbone, prompt template, and other hyperparame-\n",
      "ters (Zhang et al., 2025b). Building upon operators, a logic block (B, C) is a higher-level structural\n",
      "unit that orchestrates one or more configured operators, whereB∈ B is the logic block type, dictating\n",
      "how the operators are orchestrated. The set of available types B includes three fundamental forms of\n",
      "logic as shown in Fig. 2 (details in Appendix B):\n",
      "• SequenceLogic (seq): A sequential execution block where operators are executed one after\n",
      "another. Each operator consumes the output of its predecessor, ensuring a linear flow of\n",
      "intermediate results until the final operator produces the block output.\n",
      "• LoopLogic (for): An iterative block that repeatedly invokes its internal operators. The\n",
      "iteration continues until the stopping condition is satisfied.\n",
      "• ConditionalLogic (cond): A branching block that first executes a designated condition\n",
      "operator. Based on the evaluation outcome, it then activates one of two operator sequences.\n",
      "Only the operators in the selected branch are executed to generate the block output.\n",
      "Sequence Logic Block\n",
      "seq_block = SequenceLogic(\n",
      "    name=“b1”,\n",
      "    operators=[\n",
      "        \"generate\", \n",
      "        \"self_refine\", \n",
      "        \"test\"]\n",
      ")\n",
      "loop_block = LoopLogic(\n",
      "    name=\"b2\",\n",
      "    operators=[\n",
      "        \"generate\", \n",
      "        \"test\"],\n",
      "    max_iterations=3\n",
      ")\n",
      "cond_block = ConditionalLogic(\n",
      "    name=\"b3\",\n",
      "    condition_operator=\"test\",\n",
      "    success_operators=[\"self_refine\"],\n",
      "    failure_operators=[\"generate\"],\n",
      "    condition_field=\"result\"\n",
      ")\n",
      "Loop Logic Block\n",
      "Conditional Logic Block\n",
      "Generate Operator Self-refine Operator Test Operator\n",
      "x3\n",
      "Figure 2: The illustration of logic blocks.\n",
      "Correspondingly, C is the logic block configuration, which contains the set of configured operators\n",
      "O(D) in the block and block-level parameters (e.g., stopping condition in LoopLogic). Finally,\n",
      "the agentic workflow W is defined as a tuple W=\n",
      "\u0010\n",
      "{(Bi, Ci)}M\n",
      "i=1 , S\n",
      "\u0011\n",
      ", where M is the total\n",
      "number of logic blocks in the workflow, andS denotes the ordered sequence of logic blocks at the\n",
      "top level while each individual block may internally contain conditional or iterative control. This\n",
      "definition not only preserves the common logic patterns in code-represented workflows ensuring\n",
      "expressive diversity (Hu et al., 2025; Zhang et al., 2025b), but also enhances interpretability, including\n",
      "the explicit semantic characteristics of each logic block and the overall execution trajectory of the\n",
      "workflow, which facilitates subsequent optimization.\n",
      "Given an input query q from the dataset D which is available to every block, the execution function\n",
      "ϕexe processes workflow W by sequentially applying its logic blocks along the execution order S.\n",
      "Each block (Bi, Ci) receives the state from the previous block, a′\n",
      "i−1, and produces a new state, a′\n",
      "i,\n",
      "formally defined as:\n",
      "a′\n",
      "i =ϕ (i)\n",
      "exe(a′\n",
      "i−1, q;B i, Ci), i= 1,2, . . . , M,(1)\n",
      "where ϕ(i)\n",
      "exe is the execution function for block i and a′\n",
      "0 =∅ . The final workflow output is obtained as\n",
      "a′\n",
      "M , and then scored by the evaluation function ϕeval against the ground-truth answer a corresponding\n",
      "to q. The objective of agentic workflow optimization is to find the optimal workflow W ∗ that\n",
      "maximizes expected evaluation performance across the dataset:\n",
      "W ∗ = argmax\n",
      "W∈W\n",
      "E(q,a)∼D [ϕeval (a′\n",
      "M , a)],(2)\n",
      "whereWdenotes the search space of candidate workflows.\n",
      "4\n",
      "Dataset\n",
      "Question 1 Answer 1\n",
      "Question 2 Answer 2\n",
      "Question N Answer N\n",
      "......\n",
      "b\n",
      "a\n",
      "c\n",
      "Init Workflow\n",
      "Answer N\n",
      "Answer 1\n",
      "Answer 2\n",
      "......\n",
      "Results\n",
      "LLM\n",
      "b\n",
      "a\n",
      "c\n",
      "b\n",
      "a\n",
      "c\n",
      "......\n",
      "Judge Model\n",
      "Which logic block could \n",
      "mostly cost the error?\n",
      "b\n",
      "a\n",
      "c\n",
      "......\n",
      "ba c\n",
      "Judge Conclusion\n",
      "LLM\n",
      "Optimizer\n",
      "b\n",
      "a\n",
      "c\n",
      "b’\n",
      "a\n",
      "c\n",
      "b’ b\n",
      "a\n",
      "c\n",
      "AddModifyDelete\n",
      "b\n",
      "a\n",
      "c\n",
      "b’\n",
      "Updated Workflow\n",
      "Tree Search Update\n",
      "Pass\n",
      "Logic\n",
      "Blocks\n",
      "Answers\n",
      "LLM\n",
      "Question\n",
      "Figure 3: The main pipeline of JUDGEFLOW\n",
      "3.2 JUDGEFLOW\n",
      "Building on the representation of workflow using logic blocks, JUDGEFLOWincorporates a dedicated\n",
      "Judge module and implements an iterative Evaluation-Judge-Optimization-Update pipeline for the\n",
      "efficient optimization of agentic workflows and continues until a predefined maximum number of\n",
      "optimization rounds are met.\n",
      "3.2.1 EVALUATION-JUDGE\n",
      "The combined Evaluation-Judge stage, detailed in Algorithm 1, processes each input query from the\n",
      "dataset. If the workflow W fails on a given query, the stage identifies and logs specific problematic\n",
      "block within W . This provides targeted diagnostic signals for subsequent workflow optimization,\n",
      "enabling a more efficient and focused approach on refining these identified weak logic to improve\n",
      "overall optimization efficiency.\n",
      "Specifically, for each input queryq (with a corresponding ground-truth answer a), we have {a′\n",
      "i}M\n",
      "i=1 =\n",
      "ϕexe(q, W) , and score s=ϕ eval(a′\n",
      "M , a). The score s is recorded in a list Pscores for later calculation\n",
      "of W ’s overall performance. Providing a thresholdε that indicates successful execution, if s≥ε , the\n",
      "instance is marked as successful, and the algorithm simply proceeds to the next input.\n",
      "However, ifs < ε , a quadruple Q= (W, q, a,{a ′\n",
      "i}M\n",
      "i=1) is defined to encapsulate the full context of the\n",
      "failure. The Judge proceeds to examine the quadruple, assessing each block’s{Bi}M\n",
      "i=1 responsibility\n",
      "for the failure and ranking them accordingly. This procedure, guided by specific judging prompts\n",
      "(detailed in Appendix C), yields a rank-based score vector (Liu et al., 2025b) (ri)M\n",
      "i=1 for the blocks\n",
      "where ri = 1 refers to the block deemed most responsible for the failure and ri =M denotes the\n",
      "least responsible, each rank from 1 to M is assigned exactly once. These block scores (ri)M\n",
      "i=1 are\n",
      "appended to Rranks. The RoundWorst((ri)M\n",
      "i=1, W) function then utilizes this score vector to identify\n",
      "Brw, the block deemed most problematic for the current instance (i.e. Brw ={B i |r i = 1} ) .\n",
      "Subsequently, the instance details (q, a,{a ′\n",
      "i}M\n",
      "i=1) are logged into LBrw, the dedicated log for Brw,\n",
      "providing targeted few-shot examples for its potential future optimization.\n",
      "Upon completion of all instances in D, the accumulated diagnostic information is processed. The\n",
      "OverallWorst(Rranks, W) function analyzes all block rank-based score vectors in Rranks to identify\n",
      "Bsel, the block deemed the most consistently problematic over the whole dataset. In practice, we aggre-\n",
      "gate rank vectors across all failing instances inRranks by summing the scoresrk assigned to each block\n",
      "Bk, and then selects the block achieving the minimum sum (i.e. Bsel = arg min Bk∈W\n",
      "PT\n",
      "t=1 r(t)\n",
      "k ,\n",
      "where T is the number of the failure executions). Concurrently, the overall performance PW of\n",
      "W on D is computed by CalPerformance(Pscores). Finally, this stage returns PW , Bsel, and LBsel,\n",
      "providing actionable insights for subsequent optimization.\n",
      "5\n",
      "Algorithm 1Evaluation-Judge\n",
      "1:Input:WorkflowW, DatasetD, executorϕ exe, evaluatorϕ eval, Judge, thresholdε\n",
      "2:Output:PerformanceP W , Selected BlockB sel and the corresponding LogL Bsel\n",
      "3:Fork←1toM: InitializeL Bk ← ∅\n",
      "4:R ranks ← ∅,P scores ← ∅\n",
      "5:foreach(q, a)∈ Ddo\n",
      "6:{a ′\n",
      "i}M\n",
      "i=1 ←ϕ exe(q, W)\n",
      "7:s←ϕ eval(a′\n",
      "M , a)\n",
      "8:P scores ←APPEND(P scores, s)\n",
      "9:ifs≥εthen\n",
      "10:continue▷If success, no judging needed, skipping to next sample\n",
      "11:else\n",
      "12:(r i)M\n",
      "i=1 ←Judge(W, q, a,{a ′\n",
      "i}M\n",
      "i=1)▷Call Judge to rank blocks by responsibility\n",
      "13:R ranks ←APPEND(R ranks,(r i)M\n",
      "i=1)▷Append block-wise judge ranking\n",
      "14:B rw ←RoundWorst((r i)M\n",
      "i=1, W)▷Get the most problematic block in this round\n",
      "15:L Brw ←APPEND(L Brw,(q, a,{a ′\n",
      "i}M\n",
      "i=1))▷Append failure execution context to that block’s log\n",
      "16:end if\n",
      "17:end for\n",
      "18:B sel ←OverallWorst(R ranks, W)▷Aggregate across failures to pick the globally weakest block\n",
      "19:P W ←CalPerformance(P scores)▷Compute overall performance on the dataset\n",
      "20:returnP W , Bsel,L Bsel\n",
      "3.2.2 OPTIMIZATION-UPDATE\n",
      "In the subsequent Optimization-Update stage, the LLM-based optimizer utilizes the insights from the\n",
      "previous stage and refines W to produce an improved version W ′ guided by specific optimization\n",
      "prompts (detailed in Appendix D), which can be formally expressed as\n",
      "W ′ ←Optimizer(W, B sel, A,sample(L Bsel))(3)\n",
      "where sample(LBsel) refers to few-shot samples drawn from the logs LBsel and A∈ A , where A is a\n",
      "predefined set of available modification actions as follows:\n",
      "• Add Block: Introduce a new block Bnew with configuration Cnew, and connect it directly\n",
      "with the low-performing blockB sel;\n",
      "• Remove Block: Remove the low-performing block Bsel together with all of its incident\n",
      "edges while reconnecting its predecessor and successor to preserve sequential flow;\n",
      "•Modify Block: Reconfigure the existingB sel by updating its configurationC sel 7→C ′\n",
      "sel.\n",
      "In practice, the LLM-based optimizer selects A adaptively based on the diagnostic signals in LBsel.\n",
      "Following Zhang et al. (2025b), the refined workflowW ′ is first evaluated to obtain its performance\n",
      "score PW ′. The pair (W ′, PW ′) is then added to the candidate pool Wpool, which retains at most K\n",
      "workflows by keeping the top-Khighest-scoring entries:\n",
      "Wpool ←Top-K\n",
      "\u0000\n",
      "Wpool ∪ {(W ′, PW ′)}\n",
      "\u0001\n",
      ".(4)\n",
      "At the beginning of the next iteration, the optimizer selects a starting workflow Wstart from Wpool\n",
      "using a softmax distribution over scores with temperatureτ:\n",
      "Wstart ∼ Wpool,Pr(W i) =\n",
      "exp\n",
      "\u0010\n",
      "si−maxj sj\n",
      "τ\n",
      "\u0011\n",
      "P|Wpool|\n",
      "k=1 exp\n",
      "\u0010\n",
      "sk−maxj sj\n",
      "τ\n",
      "\u0011 ,(5)\n",
      "wheres i is the evaluation score of workflowW i.\n",
      "6\n",
      "4 EXPERIMENTS\n",
      "4.1 EXPERIMENTALSETUPS\n",
      "Benchmarks and Datasets.We evaluate our method on widely used public benchmarks, covering\n",
      "math reasoning tasks (GSM8K (Cobbe et al., 2021), MATH (Hendrycks et al., 2021)) and code\n",
      "generation tasks (MBPP (Austin et al., 2021), HumanEval (Chen et al., 2021)).\n",
      "Following previous studies (Zhang et al., 2025b;a), each dataset is divided into training and test sets\n",
      "with a ratio of 1:4. We report the solve rate (%) on GSM8K and MATH, pass@1 on MBPP and\n",
      "HumanEval as evaluation metrics.\n",
      "Baselines.We compare our JUDGEFLOWwith a series of baselines, including (1) Single-agent\n",
      "System: Standard prompting (IO), Chain-of-Thought prompting (CoT) (Wei et al., 2023), and Self-\n",
      "Consistency (Wang et al., 2023b); (2) Hand-crafted Multi-agent System: MultiPersona (Wang et al.,\n",
      "2024c), SELF-REFINE (Madaan et al., 2023), LLM-Debate (Du et al., 2023), LLM-Blender (Jiang\n",
      "et al., 2023), and DyLAN (Liu et al., 2024); (3) Autonomous Multi-agent System: GPTSwarm (Zhuge\n",
      "et al., 2024a), ADAS (Hu et al., 2025), AFlow (Zhang et al., 2025b), MaAS (Zhang et al., 2025a),\n",
      "and MermaidFlow (Zheng et al., 2025).\n",
      "Implementation Details.We use the closed-source LLM gpt-4o-mini-0718 (OpenAI, 2024b) as\n",
      "both the optimization LLM and execution LLM following the previous works Zhang et al. (2025a)\n",
      "and Zheng et al. (2025). For a fair comparison, we use the same model as Judge LLM. All the models\n",
      "are accessed via API with temperature = 0. The number of iteration rounds is set to 20 consistent with\n",
      "Zhang et al. (2025b) and Zheng et al. (2025). When optimizing, we setM≤3,ε= 1, andK= 3.\n",
      "4.2 EXPERIMENTALRESULTS\n",
      "Table 1: Performance comparison with baselines onGSM8K,MATH,MBPP, andHumanEval. The results\n",
      "are averaged over three independent runs.\n",
      "Method GSM8K MATH MBPP HumanEval Avg.\n",
      "Single-agent System\n",
      "IO 87.8 48.6 73.9 87.0 74.3\n",
      "CoT (Wei et al., 2023) 87.0 48.8 74.2 88.6 74.7\n",
      "CoT SC (Wang et al., 2023b) 86.9 50.4 73.3 91.6 75.6\n",
      "Hand-crafted Multi-agent System\n",
      "SELF-REFINE (Madaan et al., 2023) 85.5 46.1 71.8 87.8 72.8\n",
      "LLM-Debate (Du et al., 2023) 89.5 48.6 70.3 88.8 74.3\n",
      "LLM-Blender (Jiang et al., 2023) 88.4 46.9 77.1 88.7 75.3\n",
      "DyLAN (Liu et al., 2024) 90.0 48.5 77.3 90.4 76.6\n",
      "Autonomous Multi-agent System\n",
      "GPTSwarm (Zhuge et al., 2024a) 89.1 47.9 77.4 89.3 75.9\n",
      "ADAS (Hu et al., 2025) 88.4 43.2 77.1 84.2 73.2\n",
      "AFlow (Zhang et al., 2025b) 90.1 52.8 81.7 90.1 78.7\n",
      "MaAS (Zhang et al., 2025a) 91.5 52.2 82.2 91.6 79.4\n",
      "MermaidFlow (Zheng et al., 2025) 92.4 55.4 82.3 92.9 80.8\n",
      "JUDGEFLOW(Ours) 93.0 58.5 83.8 93.4 82.2\n",
      "Main Results.As shown in Table 1, JUDGEFLOWachieves superior performance compared to\n",
      "several strong baselines, including both hand-crafted and autonomous multi-agent systems consis-\n",
      "tently across all the tasks1. Notably, for some challenging benchmarks such as MATH and MBPP,\n",
      "JUDGEFLOWoutperforms the strongest prior baseline MermaidFlow by +3.1(5.6%) and +1.5(1.8%),\n",
      "1Some baseline results are referred to Zhang et al. (2025b) and Zheng et al. (2025).\n",
      "7\n",
      "respectively. At the same time, for relatively simpler benchmarks such as GSM8K and HumanEval,\n",
      "JUDGEFLOWstill achieves consistent gains of +0.6 and +0.5. Taken together, JUDGEFLOWachieves\n",
      "the average score of 82.2, representing a +1.4(1.7%) increase. The results highlight the effectiveness\n",
      "of our Judge-guided block-level optimization across both reasoning and code generation tasks.\n",
      "AFlow JudgeFlow\n",
      "35\n",
      "40\n",
      "45\n",
      "42.00%\n",
      "44.67%\n",
      "Accuracy (%)\n",
      "Figure 4: Performance on AIME 2025.\n",
      "Main Results.While several works in this area\n",
      "(Zhang et al., 2025b; Ma et al., 2025; Zheng et al.,\n",
      "2025) compare on standard benchmarks to ensure\n",
      "fair comparison, we extended our evaluation to\n",
      "significantly more challenging AIME benchmark\n",
      "(Ye et al., 2025) to demonstrate the performance\n",
      "of JUDGEFLOWon complex reasoning tasks.\n",
      "To balance performance and cost for this\n",
      "more challenging benchmark, we employed\n",
      "gpt-4.1-mini (OpenAI, 2025) as the LLM back-\n",
      "bone for the Judge, Optimization, and Execution\n",
      "modules. Following previous works and our stan-\n",
      "dard setting, the temperature was set to 0. We sam-\n",
      "pled validation data from the AIME 2024 dataset\n",
      "during the optimization phase and evaluated the\n",
      "final performance on the AIME 2025 dataset. We\n",
      "evaluated the workflows across 5 runs. As shown in Fig. 4, JudgeFlow achieves an average accuracy\n",
      "of 44.67%, improving AFlow’s absolute performance by 2.67 percentage points.\n",
      "4.3 ANALYSIS\n",
      "We take the MBPP dataset as an illustrative example to analyze JUDGEFLOW.\n",
      "Best-Performing Workflow.Fig. 5a is the best-performing workflow found by JUDGEFLOWon\n",
      "MBPP dataset. The workflow is composed of three logic blocks. First, a seq block b1 applies\n",
      "a generate operator to produce an initial candidate function. Second, a for block b2 repeatedly\n",
      "invokes the test operator until the stopping condition is satisfied. Finally, a cond block b3 runs\n",
      "the test operator to check correctness: if the candidate doesn’t pass, it routes the solution to a\n",
      "self_refineoperator for further improvement.\n",
      "Learning Curves.Fig. 5b compares the training curve and testing curve of the highest performance\n",
      "found between JUDGEFLOWand AFlow. JUDGEFLOWexhibits clear performance gains within the\n",
      "first five optimization iterations, with both the training and testing curves showing rapid improvements.\n",
      "Beyond this early stage, JUDGEFLOWcontinues to achieve gains, ultimately converging to higher\n",
      "accuracy. In contrast, AFlow remains stagnant across most iterations and only shows noticeable\n",
      "improvements in the later stage, and its final training and testing performance remain consistently\n",
      "lower than those of JUDGEFLOW.\n",
      "Impact of LLMs.According to Table 2, we keep\n",
      "gpt-4o-mini-0718 fixed as the executor LLM, while\n",
      "varying the optimization and Judge models. Par-\n",
      "ticularly, we consider gpt-4o (OpenAI, 2024a) and\n",
      "Gemini-2.5-flash (Google-Cloud, 2025) as alterna-\n",
      "tives for these roles and report the resulting performance.\n",
      "The experiment confirms that increasing the capacity of\n",
      "optimization and Judge models consistently improves\n",
      "performance. While all models yield competitive re-\n",
      "sults, GPT-4o attains the best score 84.5.\n",
      "Table 2: Testing performance using different\n",
      "LLMs on MBPP dataset.\n",
      "Models Score\n",
      "GPT-4o-mini 83.8\n",
      "GPT-4o 84.5\n",
      "Gemini-2.5-flash 84.4\n",
      "8\n",
      "END\n",
      "IF\n",
      "b3:cond\n",
      "b1:\n",
      "seq\n",
      "b2:\n",
      "for\n",
      "Stop?\n",
      "Generator\n",
      "Test\n",
      "Self-Refine\n",
      "(a)\n",
      "1 5 10 15 200.7\n",
      "0.75\n",
      "0.8\n",
      "0.85\n",
      "0.9\n",
      "Iteration Number\n",
      "pass@1 score\n",
      "JUDGEFLOW(Train)JUDGEFLOW(Test)AFlow (Train) AFlow (Test)\n",
      "(b)\n",
      "Figure 5: Fig. 5a The optimal workflow found by JUDGEFLOWon MBPP dataset; Fig. 5b The training and\n",
      "testing curve between JUDGEFLOWand AFlow on MBPP dataset.\n",
      "Optimization Efficiency.We perform an ablation\n",
      "study on different key components of JUDGEFLOW.\n",
      "As shown in Table 3, removing the logic block ab-\n",
      "straction or the judge module leads to consistent perfor-\n",
      "mance drops, confirming the importance of both design\n",
      "choices.\n",
      "Table 3: Ablation results on MBPP.\n",
      "Method Score\n",
      "JUDGEFLOW83.8\n",
      "- Logic Block 81.8\n",
      "- Judge 80.6\n",
      "Cross-benchmark Generalization.To demonstrate the generalization of JUDGEFLOW, we evaluate\n",
      "the cross-benchmark transferability, including the Math transfer and Code transfer. We optimize the\n",
      "workflow based on the MATH(MBPP) dataset and zero-shot evaluation on GSM8K(HumanEval)\n",
      "dataset. We report the solve rate (%) on GSM8K and pass@1 on HumanEval as evaluation metrics in\n",
      "Table 4, showing JUDGEFLOWyields state-of-the-art results.\n",
      "Table 4: Cross-Benchmark Transfer Performance\n",
      "Optimization→Evaluation MATH→GSM8K MBPP→HumanEval\n",
      "AFlow 91.95 90.84\n",
      "JUDGEFLOW 92.89 93.89\n",
      "JudgeTo ensure robustness against noisy outputs, JUDGEFLOWemploys a statistical filtering\n",
      "mechanism. Instead of a single potentially noisy judge output, JUDGEFLOWcollects traces from\n",
      "all failed instances and aggregates responsibility scores using the OverallWorst mechanism. We\n",
      "follow the recent finding in (Zhang et al., 2025c); while individual LLM-based failure attribution\n",
      "might contain noise, the aggregated distribution of responsible logic blocks is more consistent with\n",
      "the true causes of failure.\n",
      "Regarding computational cost, although JUDGEFLOWintroduces an additional Judge module for\n",
      "LLM calls, our analysis reveals that the dominant cost in agentic workflow optimization lies in the\n",
      "Evaluation phase rather than the Judge module in our proposed methods. For example, we monitor\n",
      "the cost for a single optimization round on the GSM8K dataset, and the Evaluation costs $0.45 while\n",
      "the Judge costs $0.01, the ratio of Judge/Evaluation is about 2%.\n",
      "Logic BlockThe Logic Block serves as a critical abstraction designed to enable the Judge mecha-\n",
      "nism. At the operator level, operators often interact through complex control flows (e.g., theif-else\n",
      "structure in blockb3 of Fig. 5a), making it difficult to attribute blame to a single operator because\n",
      "execution paths vary dynamically per query (e.g., an operator in an unexecuted else branch). By\n",
      "introducing the Logic Blocks, we encapsulate these dynamic control flows into a stable semantic unit.\n",
      "This abstraction mitigates the ambiguity caused by varying execution paths, thereby enabling the\n",
      "Judge to perform stable and effective attribution.\n",
      "9\n",
      "4.4 CASESTUDY\n",
      "To illustrate how JUDGEFLOWworks in practice, we present a case study of workflow optimization\n",
      "on the GSM8K dataset. This example demonstrates how JUDGEFLOWautomatically identifies and\n",
      "rectifies a suboptimal workflow through the pipeline, as shown in Fig. 6. The initial workflow consists\n",
      "of two logic blocks:b1, aseq block consisting of onemulti_generate_ensemble operator designed\n",
      "to generate and ensemble multiple candidate solutions (with num_solutions set to 3), and b2, a seq\n",
      "block consisting of oneprogrammer operator, which takes the output from the previous block and\n",
      "generates the final answer using programming. When processing a batch of GSM8K instances, this\n",
      "workflow failed multiple times, triggering the Evaluation-Judge stage. The Judge module analyzed\n",
      "execution traces of these failures and assigned rank-based responsibility scores to each block. For\n",
      "example, in one failed run, it output{\"b2\": 1, \"b1\": 2}, attributing the primary blame tob2, while\n",
      "in another it output{\"b1\": 1, \"b2\": 2}, assigning higher responsibility tob1. By aggregating these\n",
      "rank-based scores across failures, the system identifiedb1 as theOverallWorst block, indicating that\n",
      "low-quality initial solutions fromb1 were the main bottleneck, making it difficult for the workflow to\n",
      "generate correct final answers.\n",
      "In the Optimization-Update stage, the LLM-based Optimizer received this diagnostic signal and\n",
      "selected theAdd Blockaction. It introduced a new logic block, b3, of type seq, with operator\n",
      "self_refine, which iteratively improves candidate solutions. This block was inserted betweenb1\n",
      "andb2, producing the new workflow[\"b1\", \"b3\", \"b2\"]. The updated workflow first generates\n",
      "multiple ideas withb1, then refines them withb3, and finally produces the polished answer through\n",
      "b2. This case study highlights the strength of JUDGEFLOW: instead of relying solely on end-to-end\n",
      "success signals, it leverages block-level diagnostics from the Judge to perform precise error attribution,\n",
      "enabling workflow modifications that directly address weaknesses. As a result, JUDGEFLOWavoids\n",
      "blind search, achieves more efficient optimization, and substantially improves performance.\n",
      "\"operators\": {\n",
      "    \"multi_generate_ensemble\": {\n",
      "        \"type\": \"multi_generate_ensemble\"\n",
      "    },\n",
      "    \"programmer\": {\n",
      "        \"type\": \"programmer\"\n",
      "    },\n",
      "    \"multi_generate_ensemble_v2\": {\n",
      "        \"type\": \"multi_generate_ensemble\",\n",
      "        \"num_solutions\": 3\n",
      "    },\n",
      "    \"self_refine\": {\n",
      "            \"type\": \"self_refine\"\n",
      "    }}\n",
      "+\n",
      "+\n",
      "+\n",
      "+\n",
      "+\n",
      "+\n",
      "+\n",
      "-\n",
      "LLM\n",
      "Data\n",
      "\"blocks\": [\n",
      "    {   \"name\": \"b1\",\n",
      "        \"type\": \"seq\",\n",
      "        \"operators\": [\"multi_generate_ensemble_v2\"]\n",
      "    },\n",
      "    {   \"name\": \"b2\",\n",
      "        \"type\": \"seq\",\n",
      "        \"operators\": [\"programmer\"]\n",
      "    },\n",
      "    {   \"name\": \"b3\",\n",
      "        \"type\": \"seq\",\n",
      "        \"operators\": [\"self_refine\"]\n",
      "    }],\n",
      "\"workflow\": [\"b1\", \"b2\"]}\n",
      "\"workflow\": [\"b1\", \"b3\", \"b2\"]\n",
      "..#\n",
      "{“b1”: 2, “b2”: 1}\n",
      "{“b1”: 1, “b2”: 2}\n",
      "..# ..#\n",
      "ba\n",
      "Figure 6: The illustration of the case study in the GSM8K dataset.\n",
      "5 CONCLUSION\n",
      "In this paper, we presented a novel Evaluation-Judge-Optimization-Update pipeline named JUDGE-\n",
      "FLOWfor automating the optimization of agentic workflows. By introducing reusable logic blocks as\n",
      "higher-level structural abstractions, JUDGEFLOWachieves a balance between the expressive flexibility\n",
      "of code-based workflows and the tractability of optimization. On top of this representation, the Judge\n",
      "module provides block-level diagnostic signals by analyzing execution traces and assigning respon-\n",
      "sibility to problematic block, enabling more interpretable and fine-grained optimization. Through\n",
      "extensive experiments on mathematical reasoning and code generation benchmarks, we demonstrate\n",
      "that JUDGEFLOWconsistently outperforms strong baselines. While achieving success in optimizing\n",
      "agentic workflows, LLM-as-a-Judge can be biased and may provide misleading responsibility scores.\n",
      "Future work may include exploring more robust Judge for agentic systems optimization, such as\n",
      "statistical signals or other validation methods.\n",
      "10\n",
      "REFERENCES\n",
      "Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan,\n",
      "Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, and Charles Sutton. Program Synthesis with Large\n",
      "Language Models, August 2021. URL http://arxiv.org/abs/2108.07732. arXiv:2108.07732\n",
      "[cs].\n",
      "Rishi Bommasani, Drew A. Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx,\n",
      "Michael S. Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, Erik Brynjolfsson,\n",
      "Shyamal Buch, Dallas Card, Rodrigo Castellon, Niladri Chatterji, Annie Chen, Kathleen Creel,\n",
      "Jared Quincy Davis, Dora Demszky, Chris Donahue, Moussa Doumbouya, Esin Durmus, Stefano\n",
      "Ermon, John Etchemendy, Kawin Ethayarajh, Li Fei-Fei, Chelsea Finn, Trevor Gale, Lauren\n",
      "Gillespie, Karan Goel, Noah Goodman, Shelby Grossman, Neel Guha, Tatsunori Hashimoto, Peter\n",
      "Henderson, John Hewitt, Daniel E. Ho, Jenny Hong, Kyle Hsu, Jing Huang, Thomas Icard, Saahil\n",
      "Jain, Dan Jurafsky, Pratyusha Kalluri, Siddharth Karamcheti, Geoff Keeling, Fereshte Khani, Omar\n",
      "Khattab, Pang Wei Koh, Mark Krass, Ranjay Krishna, Rohith Kuditipudi, Ananya Kumar, Faisal\n",
      "Ladhak, Mina Lee, Tony Lee, Jure Leskovec, Isabelle Levent, Xiang Lisa Li, Xuechen Li, Tengyu\n",
      "Ma, Ali Malik, Christopher D. Manning, Suvir Mirchandani, Eric Mitchell, Zanele Munyikwa,\n",
      "Suraj Nair, Avanika Narayan, Deepak Narayanan, Ben Newman, Allen Nie, Juan Carlos Niebles,\n",
      "Hamed Nilforoshan, Julian Nyarko, Giray Ogut, Laurel Orr, Isabel Papadimitriou, Joon Sung\n",
      "Park, Chris Piech, Eva Portelance, Christopher Potts, Aditi Raghunathan, Rob Reich, Hongyu\n",
      "Ren, Frieda Rong, Yusuf Roohani, Camilo Ruiz, Jack Ryan, Christopher Ré, Dorsa Sadigh,\n",
      "Shiori Sagawa, Keshav Santhanam, Andy Shih, Krishnan Srinivasan, Alex Tamkin, Rohan Taori,\n",
      "Armin W. Thomas, Florian Tramèr, Rose E. Wang, William Wang, Bohan Wu, Jiajun Wu, Yuhuai\n",
      "Wu, Sang Michael Xie, Michihiro Yasunaga, Jiaxuan You, Matei Zaharia, Michael Zhang, Tianyi\n",
      "Zhang, Xikun Zhang, Yuhui Zhang, Lucia Zheng, Kaitlyn Zhou, and Percy Liang. On the\n",
      "Opportunities and Risks of Foundation Models, July 2022. URL http://arxiv.org/abs/2108.\n",
      "07258. arXiv:2108.07258 [cs].\n",
      "Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal,\n",
      "Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel\n",
      "Herbert-V oss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler,\n",
      "Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott\n",
      "Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya\n",
      "Sutskever, and Dario Amodei. Language Models are Few-Shot Learners, July 2020. URL\n",
      "http://arxiv.org/abs/2005.14165. arXiv:2005.14165 [cs].\n",
      "Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared\n",
      "Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri,\n",
      "Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan,\n",
      "Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian,\n",
      "Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios\n",
      "Chantzis, Elizabeth Barnes, Ariel Herbert-V oss, William Hebgen Guss, Alex Nichol, Alex Paino,\n",
      "Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders,\n",
      "Christopher Hesse, Andrew N. Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan Morikawa,\n",
      "Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob\n",
      "McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba. Evaluating\n",
      "Large Language Models Trained on Code, July 2021. URLhttp://arxiv.org/abs/2107.03374.\n",
      "arXiv:2107.03374 [cs].\n",
      "Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser,\n",
      "Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John\n",
      "Schulman. Training Verifiers to Solve Math Word Problems, November 2021. URL http:\n",
      "//arxiv.org/abs/2110.14168. arXiv:2110.14168 [cs].\n",
      "Yilun Du, Shuang Li, Antonio Torralba, Joshua B. Tenenbaum, and Igor Mordatch. Improving\n",
      "Factuality and Reasoning in Language Models through Multiagent Debate, May 2023. URL\n",
      "http://arxiv.org/abs/2305.14325.\n",
      "Chrisantha Fernando, Dylan Banarse, Henryk Michalewski, Simon Osindero, and Tim Rocktäschel.\n",
      "Promptbreeder: Self-Referential Self-Improvement Via Prompt Evolution, September 2023. URL\n",
      "http://arxiv.org/abs/2309.16797.\n",
      "11\n",
      "Google-Cloud. Gemini 2.5 flash| vertex ai. https://cloud.google.com/vertex-ai/\n",
      "generative-ai/docs/models/gemini/2-5-flash, 2025. Accessed: 2025-05-18.\n",
      "Jiawei Gu, Xuhui Jiang, Zhichao Shi, Hexiang Tan, Xuehao Zhai, Chengjin Xu, Wei Li, Yinghan\n",
      "Shen, Shengjie Ma, Honghao Liu, Saizhuo Wang, Kun Zhang, Yuanzhuo Wang, Wen Gao, Lionel\n",
      "Ni, and Jian Guo. A Survey on LLM-as-a-Judge, March 2025. URL http://arxiv.org/abs/\n",
      "2411.15594.\n",
      "Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn\n",
      "Song, and Jacob Steinhardt. Measuring Mathematical Problem Solving With the MATH Dataset,\n",
      "November 2021. URLhttp://arxiv.org/abs/2103.03874. arXiv:2103.03874 [cs].\n",
      "Sirui Hong, Mingchen Zhuge, Jiaqi Chen, Xiawu Zheng, Yuheng Cheng, Ceyao Zhang, Jinlin Wang,\n",
      "Zili Wang, Steven Ka Shing Yau, Zijuan Lin, Liyang Zhou, Chenyu Ran, Lingfeng Xiao, Chenglin\n",
      "Wu, and Jürgen Schmidhuber. MetaGPT: Meta Programming for A Multi-Agent Collaborative\n",
      "Framework, November 2024. URLhttp://arxiv.org/abs/2308.00352.\n",
      "Shengran Hu, Cong Lu, and Jeff Clune. Automated Design of Agentic Systems, March 2025. URL\n",
      "http://arxiv.org/abs/2408.08435.\n",
      "Xu Huang, Weiwen Liu, Xiaolong Chen, Xingmei Wang, Hao Wang, Defu Lian, Yasheng Wang,\n",
      "Ruiming Tang, and Enhong Chen. Understanding the planning of LLM agents: A survey, February\n",
      "2024. URLhttp://arxiv.org/abs/2402.02716. arXiv:2402.02716 [cs].\n",
      "Frank Hutter, Lars Kotthoff, and Joaquin Vanschoren.Automated Machine Learning: Methods,\n",
      "Systems, Challenges. Springer Publishing Company, Incorporated, 1st edition, 2019. ISBN\n",
      "3030053172.\n",
      "Dongfu Jiang, Xiang Ren, and Bill Yuchen Lin. Llm-blender: Ensembling large language models\n",
      "with pairwise ranking and generative fusion, 2023. URL https://arxiv.org/abs/2306.02561.\n",
      "Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon Child,\n",
      "Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling Laws for Neural Language\n",
      "Models, January 2020. URLhttp://arxiv.org/abs/2001.08361. arXiv:2001.08361 [cs].\n",
      "Yu-Ang Lee, Guan-Ting Yi, Mei-Yi Liu, Jui-Chao Lu, Guan-Bo Yang, and Yun-Nung Chen. Com-\n",
      "pound AI Systems Optimization: A Survey of Methods, Challenges, and Future Directions, June\n",
      "2025. URLhttp://arxiv.org/abs/2506.08234. arXiv:2506.08234 [cs].\n",
      "Guohao Li, Hasan Abed Al Kader Hammoud, Hani Itani, Dmitrii Khizbullin, and Bernard Ghanem.\n",
      "CAMEL: Communicative Agents for \"Mind\" Exploration of Large Language Model Society,\n",
      "November 2023. URLhttp://arxiv.org/abs/2303.17760.\n",
      "Haitao Li, Qian Dong, Junjie Chen, Huixue Su, Yujia Zhou, Qingyao Ai, Ziyi Ye, and Yiqun Liu.\n",
      "LLMs-as-Judges: A Comprehensive Survey on LLM-based Evaluation Methods, December 2024.\n",
      "URLhttp://arxiv.org/abs/2412.05579.\n",
      "Wenwu Li, Xiangfeng Wang, Wenhao Li, and Bo Jin. A Survey of Automatic Prompt Engineering:\n",
      "An Optimization Perspective, February 2025. URLhttp://arxiv.org/abs/2502.11560.\n",
      "Bang Liu, Xinfeng Li, Jiayi Zhang, Jinlin Wang, Tanjin He, Sirui Hong, Hongzhang Liu, Shaokun\n",
      "Zhang, Kaitao Song, Kunlun Zhu, Yuheng Cheng, Suyuchen Wang, Xiaoqiang Wang, Yuyu Luo,\n",
      "Haibo Jin, Peiyan Zhang, Ollie Liu, Jiaqi Chen, Huan Zhang, Zhaoyang Yu, Haochen Shi, Boyan\n",
      "Li, Dekun Wu, Fengwei Teng, Xiaojun Jia, Jiawei Xu, Jinyu Xiang, Yizhang Lin, Tianming Liu,\n",
      "Tongliang Liu, Yu Su, Huan Sun, Glen Berseth, Jianyun Nie, Ian Foster, Logan Ward, Qingyun\n",
      "Wu, Yu Gu, Mingchen Zhuge, Xiangru Tang, Haohan Wang, Jiaxuan You, Chi Wang, Jian Pei,\n",
      "Qiang Yang, Xiaoliang Qi, and Chenglin Wu. Advances and Challenges in Foundation Agents:\n",
      "From Brain-Inspired Intelligence to Evolutionary, Collaborative, and Safe Systems, March 2025a.\n",
      "URLhttp://arxiv.org/abs/2504.01990.\n",
      "Yinhong Liu, Han Zhou, Zhijiang Guo, Ehsan Shareghi, Ivan Vuli ´c, Anna Korhonen, and Nigel\n",
      "Collier. Aligning with Human Judgement: The Role of Pairwise Preference in Large Language\n",
      "Model Evaluators, January 2025b. URLhttp://arxiv.org/abs/2403.16950.\n",
      "12\n",
      "Zijun Liu, Yanzhe Zhang, Peng Li, Yang Liu, and Diyi Yang. A Dynamic LLM-Powered Agent\n",
      "Network for Task-Oriented Agent Collaboration, November 2024. URLhttp://arxiv.org/abs/\n",
      "2310.02170.\n",
      "Xiaowen Ma, Chenyang Lin, Yao Zhang, V olker Tresp, and Yunpu Ma. Agentic Neural Networks:\n",
      "Self-Evolving Multi-Agent Systems via Textual Backpropagation, July 2025. URLhttp://arxiv.\n",
      "org/abs/2506.09046. arXiv:2506.09046 [cs].\n",
      "Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon,\n",
      "Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, Shashank Gupta, Bodhisattwa Prasad Majumder,\n",
      "Katherine Hermann, Sean Welleck, Amir Yazdanbakhsh, and Peter Clark. Self-refine: Iterative\n",
      "refinement with self-feedback. InThirty-seventh Conference on Neural Information Processing\n",
      "Systems, 2023. URLhttps://openreview.net/forum?id=S37hOerQLB.\n",
      "OpenAI. Gpt-4o | openai platform documentation.https://platform.openai.com/docs/models/\n",
      "gpt-4o, 2024a.\n",
      "OpenAI. Gpt-4o-mini | openai platform documentation. https://platform.openai.com/docs/\n",
      "models/gpt-4o-mini, 2024b. Accessed: 2025-05-18.\n",
      "OpenAI. Gpt-4_1-mini | openai platform documentation. https://platform.openai.com/docs/\n",
      "models/gpt-4.1-mini, 2025. Accessed: 2025-11-27.\n",
      "Reid Pryzant, Dan Iter, Jerry Li, Yin Lee, Chenguang Zhu, and Michael Zeng. Automatic Prompt\n",
      "Optimization with “Gradient Descent” and Beam Search. In Houda Bouamor, Juan Pino, and\n",
      "Kalika Bali (eds.),Proceedings of the 2023 Conference on Empirical Methods in Natural Language\n",
      "Processing, pp. 7957–7968, Singapore, December 2023. Association for Computational Linguistics.\n",
      "doi: 10.18653/v1/2023.emnlp-main.494. URL https://aclanthology.org/2023.emnlp-main.\n",
      "494/.\n",
      "Pranav Putta, Edmund Mills, Naman Garg, Sumeet Motwani, Chelsea Finn, Divyansh Garg, and\n",
      "Rafael Rafailov. Agent Q: Advanced Reasoning and Learning for Autonomous AI Agents, August\n",
      "2024. URLhttp://arxiv.org/abs/2408.07199. arXiv:2408.07199 [cs].\n",
      "Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher D. Manning, and Chelsea\n",
      "Finn. Direct Preference Optimization: Your Language Model is Secretly a Reward Model,\n",
      "December 2023. URLhttp://arxiv.org/abs/2305.18290.\n",
      "Kiran Ramnath, Kang Zhou, Sheng Guan, Soumya Smruti Mishra, Xuan Qi, Zhengyuan Shen, Shuai\n",
      "Wang, Sangmin Woo, Sullam Jeoung, Yawei Wang, Haozhu Wang, Han Ding, Yuzhe Lu, Zhichao\n",
      "Xu, Yun Zhou, Balasubramaniam Srinivasan, Qiaojing Yan, Yueyan Chen, Haibo Ding, Panpan\n",
      "Xu, and Lin Lee Cheong. A Systematic Survey of Automatic Prompt Optimization Techniques,\n",
      "April 2025. URLhttp://arxiv.org/abs/2502.16923. arXiv:2502.16923 [cs].\n",
      "Yu Shang, Yu Li, Keyu Zhao, Likai Ma, Jiahe Liu, Fengli Xu, and Yong Li. AgentSquare: Automatic\n",
      "LLM Agent Search in Modular Design Space, October 2024. URL http://arxiv.org/abs/\n",
      "2410.06153.\n",
      "Jinwei Su, Yinghui Xia, Ronghua Shi, Jianhui Wang, Jianuo Huang, Yijin Wang, Tianyu Shi, Yang\n",
      "Jingsong, and Lewei He. DebFlow: Automating Agent Creation via Agent Debate, March 2025.\n",
      "URLhttp://arxiv.org/abs/2503.23781. arXiv:2503.23781 [cs].\n",
      "Khanh-Tung Tran, Dung Dao, Minh-Duong Nguyen, Quoc-Viet Pham, Barry O’Sullivan, and\n",
      "Hoang D. Nguyen. Multi-Agent Collaboration Mechanisms: A Survey of LLMs, January 2025.\n",
      "URLhttp://arxiv.org/abs/2501.06322.\n",
      "Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen Zhang, Zhiyuan Chen, Jiakai\n",
      "Tang, Xu Chen, Yankai Lin, Wayne Xin Zhao, Zhewei Wei, and Jirong Wen. A survey on large\n",
      "language model based autonomous agents.Frontiers of Computer Science, 18(6), December\n",
      "2024a. ISSN 2095-2228, 2095-2236. doi: 10.1007/s11704-024-40231-1. URL https://link.\n",
      "springer.com/10.1007/s11704-024-40231-1.\n",
      "13\n",
      "Peiyi Wang, Lei Li, Liang Chen, Zefan Cai, Dawei Zhu, Binghuai Lin, Yunbo Cao, Qi Liu, Tianyu\n",
      "Liu, and Zhifang Sui. Large Language Models are not Fair Evaluators, August 2023a. URL\n",
      "http://arxiv.org/abs/2305.17926. arXiv:2305.17926 [cs].\n",
      "Wenyi Wang, Hisham A. Alyahya, Dylan R. Ashley, Oleg Serikov, Dmitrii Khizbullin, Francesco\n",
      "Faccio, and Jürgen Schmidhuber. How to Correctly do Semantic Backpropagation on Language-\n",
      "based Agentic Systems, December 2024b. URLhttp://arxiv.org/abs/2412.03624.\n",
      "Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdh-\n",
      "ery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models,\n",
      "2023b. URLhttps://arxiv.org/abs/2203.11171.\n",
      "Zhenhailong Wang, Shaoguang Mao, Wenshan Wu, Tao Ge, Furu Wei, and Heng Ji. Unleashing the\n",
      "emergent cognitive synergy in large language models: A task-solving agent through multi-persona\n",
      "self-collaboration, 2024c. URLhttps://arxiv.org/abs/2307.05300.\n",
      "Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le,\n",
      "and Denny Zhou. Chain-of-thought prompting elicits reasoning in large language models, 2023.\n",
      "URLhttps://arxiv.org/abs/2201.11903.\n",
      "Qingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Beibin Li, Erkang Zhu, Li Jiang, Xiaoyun\n",
      "Zhang, Shaokun Zhang, Jiale Liu, Ahmed Hassan Awadallah, Ryen W. White, Doug Burger,\n",
      "and Chi Wang. AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation,\n",
      "October 2023. URLhttp://arxiv.org/abs/2308.08155.\n",
      "Shirley Wu, Shiyu Zhao, Qian Huang, Kexin Huang, Michihiro Yasunaga, Kaidi Cao, Vassilis N\n",
      "Ioannidis, Karthik Subbian, Jure Leskovec, and James Zou. A V ATAR: Optimizing LLM Agents\n",
      "for Tool Usage via Contrastive Reasoning. 2024.\n",
      "Jinyu Xiang, Jiayi Zhang, Zhaoyang Yu, Fengwei Teng, Jinhao Tu, Xinbing Liang, Sirui Hong,\n",
      "Chenglin Wu, and Yuyu Luo. Self-Supervised Prompt Optimization, February 2025. URL\n",
      "http://arxiv.org/abs/2502.06855.\n",
      "Chengrun Yang, Xuezhi Wang, Yifeng Lu, Hanxiao Liu, Quoc V . Le, Denny Zhou, and Xinyun Chen.\n",
      "Large Language Models as Optimizers, April 2024. URL http://arxiv.org/abs/2309.03409.\n",
      "Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William W. Cohen, Ruslan Salakhutdinov,\n",
      "and Christopher D. Manning. Hotpotqa: A dataset for diverse, explainable multi-hop question\n",
      "answering, 2018. URLhttps://arxiv.org/abs/1809.09600.\n",
      "Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L. Griffiths, Yuan Cao, and Karthik\n",
      "Narasimhan. Tree of Thoughts: Deliberate Problem Solving with Large Language Models,\n",
      "December 2023a. URLhttp://arxiv.org/abs/2305.10601. arXiv:2305.10601 [cs].\n",
      "Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao.\n",
      "ReAct: Synergizing Reasoning and Acting in Language Models, March 2023b. URL http:\n",
      "//arxiv.org/abs/2210.03629. arXiv:2210.03629 [cs].\n",
      "Yixin Ye, Yang Xiao, Tiantian Mi, and Pengfei Liu. Aime-preview: A rigorous and immediate\n",
      "evaluation framework for advanced mathematical reasoning.https://github.com/GAIR-NLP/\n",
      "AIME-Preview, 2025. GitHub repository.\n",
      "Li Yin and Zhangyang Wang. LLM-AutoDiff: Auto-Differentiate Any LLM Workflow, January\n",
      "2025. URLhttp://arxiv.org/abs/2501.16673.\n",
      "Xunjian Yin, Xinyi Wang, Liangming Pan, Xiaojun Wan, and William Yang Wang. Gödel Agent:\n",
      "A Self-Referential Agent Framework for Recursive Self-Improvement, February 2025. URL\n",
      "http://arxiv.org/abs/2410.04444.\n",
      "Mert Yuksekgonul, Federico Bianchi, Joseph Boen, Sheng Liu, Zhi Huang, Carlos Guestrin, and\n",
      "James Zou. TextGrad: Automatic \"Differentiation\" via Text, June 2024. URL http://arxiv.\n",
      "org/abs/2406.07496.\n",
      "14\n",
      "Guibin Zhang, Luyang Niu, Junfeng Fang, Kun Wang, Lei Bai, and Xiang Wang. Multi-agent\n",
      "Architecture Search via Agentic Supernet, February 2025a. URL http://arxiv.org/abs/2502.\n",
      "04180.\n",
      "Jiayi Zhang, Jinyu Xiang, Zhaoyang Yu, Fengwei Teng, Xionghui Chen, Jiaqi Chen, Mingchen\n",
      "Zhuge, Xin Cheng, Sirui Hong, Jinlin Wang, Bingnan Zheng, Bang Liu, Yuyu Luo, and Chenglin\n",
      "Wu. AFlow: Automating Agentic Workflow Generation, February 2025b. URLhttp://arxiv.\n",
      "org/abs/2410.10762.\n",
      "Shaokun Zhang, Ming Yin, Jieyu Zhang, Jiale Liu, Zhiguang Han, Jingyang Zhang, Beibin Li, Chi\n",
      "Wang, Huazheng Wang, Yiran Chen, and Qingyun Wu. Which Agent Causes Task Failures and\n",
      "When? On Automated Failure Attribution of LLM Multi-Agent Systems, April 2025c. URL\n",
      "http://arxiv.org/pdf/2505.00212. arXiv:2505.00212 [cs].\n",
      "Chengqi Zheng, Jianda Chen, Yueming Lyu, Wen Zheng Terence Ng, Haopeng Zhang, Yew-Soon Ong,\n",
      "Ivor Tsang, and Haiyan Yin. MermaidFlow: Redefining Agentic Workflow Generation via Safety-\n",
      "Constrained Evolutionary Programming, May 2025. URL http://arxiv.org/abs/2505.22967.\n",
      "arXiv:2505.22967 [cs].\n",
      "Wangchunshu Zhou, Yixin Ou, Shengwei Ding, Long Li, Jialong Wu, Tiannan Wang, Jiamin Chen,\n",
      "Shuai Wang, Xiaohua Xu, Ningyu Zhang, Huajun Chen, and Yuchen Eleanor Jiang. Symbolic\n",
      "Learning Enables Self-Evolving Agents, June 2024. URL http://arxiv.org/abs/2406.18532.\n",
      "Mingchen Zhuge, Wenyi Wang, Louis Kirsch, Francesco Faccio, Dmitrii Khizbullin, and Jürgen\n",
      "Schmidhuber. Language Agents as Optimizable Graphs, August 2024a. URL http://arxiv.\n",
      "org/abs/2402.16823.\n",
      "Mingchen Zhuge, Changsheng Zhao, Dylan Ashley, Wenyi Wang, Dmitrii Khizbullin, Yunyang\n",
      "Xiong, Zechun Liu, Ernie Chang, Raghuraman Krishnamoorthi, Yuandong Tian, Yangyang Shi,\n",
      "Vikas Chandra, and Jürgen Schmidhuber. Agent-as-a-Judge: Evaluate Agents with Agents, October\n",
      "2024b. URLhttp://arxiv.org/abs/2410.10934.\n",
      "Henry Peng Zou, Wei-Chieh Huang, Yaozu Wu, Yankai Chen, Chunyu Miao, Hoang Nguyen, Yue\n",
      "Zhou, Weizhi Zhang, Liancheng Fang, Langzhou He, Yangning Li, Dongyuan Li, Renhe Jiang,\n",
      "Xue Liu, and Philip S. Yu. LLM-Based Human-Agent Collaboration and Interaction Systems: A\n",
      "Survey, June 2025. URLhttp://arxiv.org/abs/2505.00753. arXiv:2505.00753 [cs].\n",
      "15\n",
      "A OPERATORS\n",
      "Following Zhang et al. (2025b), Zhang et al. (2025a) and Zheng et al. (2025), we adopt the following\n",
      "set of operators:\n",
      "1.generate , a generation operator that produces candidate solutions based on the problem\n",
      "description and optional previous results.\n",
      "2.test , a testing operator that executes generated solutions against test cases and provides\n",
      "feedback for refinement.\n",
      "3.self_refine , a refinement operator that improves a given solution through self-refinement.\n",
      "4.multi_generate_ensemble , an ensemble operator that generates multiple solutions and\n",
      "combine them to the best one via self-consistency.\n",
      "5.programmer , a synthesis-and-execution operator that generates Python code for solving\n",
      "math problems, runs it in a restricted environment, and iteratively repairs errors.\n",
      "B LOGICBLOCKS\n",
      "We implement three common logic types in code-represented workflows:SequenceLogic ( seq),\n",
      "LoopLogic (for), andConditionalLogic ( cond), whose descriptions and interfaces are listed below.\n",
      "Logic Blocks\n",
      "{\n",
      "\"SequenceLogic\": {\n",
      "\"type\": \"seq\",\n",
      "\"description\": \"Execute operators strictly in order. Required\n",
      "fields: name (string), type (must be'seq'), operators (\n",
      "array of operator aliases). No optional fields. Use this\n",
      "for linear processing flows where you need sequential\n",
      "execution of operators.\",\n",
      "\"structure\": {\n",
      "\"name\": \"block_name\",\n",
      "\"type\": \"seq\",\n",
      "\"operators\": [\"operator\"]\n",
      "},\n",
      "\"input_flow\": \"block_input -> op1 -> op2 -> ... ->\n",
      "block_output\"\n",
      "},\n",
      "\"LoopLogic\": {\n",
      "\"type\": \"for\",\n",
      "\"description\": \"Iteratively execute a sequence of operators\n",
      "until the optional asynchronous condition returns False or\n",
      "the max iteration limit is reached. Required fields: name\n",
      "(string), type (must be'for'), operators (array of\n",
      "operator aliases). Optional fields: max_iterations (\n",
      "integer, default 3), condition (object with'field'and'\n",
      "equals'properties, or null for no condition). Use this\n",
      "for retry mechanisms and iterative refinement.\",\n",
      "\"structure\": {\n",
      "\"name\": \"block_name\",\n",
      "\"type\": \"for\",\n",
      "\"operators\": [\"operator\"],\n",
      "\"max_iterations\": num_iterations,\n",
      "\"condition\": {\n",
      "\"field\": \"field_name\",\n",
      "\"equals\":\n",
      "}\n",
      "},\n",
      "\"input_flow\": \"block_input -> repeat [op1 -> op2 -> ...] until\n",
      "stop -> block_output\"\n",
      "16\n",
      "},\n",
      "\"ConditionalLogic\": {\n",
      "\"type\": \"cond\",\n",
      "\"description\": \"Run a dedicated condition operator first, then\n",
      "choose the success or failure branch based on the field\n",
      "specified by'condition_field'. The chosen branch runs\n",
      "sequentially with the same data-passing semantics as\n",
      "SequenceLogic. Required fields: name (string), type (must\n",
      "be'cond'), condition_operator (string, operator alias to\n",
      "evaluate condition), success_operators (array of operator\n",
      "aliases for success path), failure_operators (array of\n",
      "operator aliases for failure path). Optional fields:\n",
      "condition_field (string, field name to check for condition\n",
      "result, default'result'). The condition operator\n",
      "evaluates criteria and sets a result field, which\n",
      "determines whether to execute success_operators or\n",
      "failure_operators. Use this for branching logic and\n",
      "conditional processing. \",\n",
      "\"structure\": {\n",
      "\"name\": \"block_name\",\n",
      "\"type\": \"cond\",\n",
      "\"condition_operator\": \"condition_operator\",\n",
      "\"success_operators\": [\"success_op\"],\n",
      "\"failure_operators\": [\"failure_op\"],\n",
      "\"condition_field\": \"field_name\"\n",
      "},\n",
      "\"input_flow\": \"block_input -> condition operator -> select\n",
      "branch -> branch sequence -> block_output\"\n",
      "}\n",
      "}\n",
      "C JUDGEPROMPT\n",
      "Judge Prompt\n",
      "You are a workflow failure analyst. Given execution evidence from a block-based AI\n",
      "workflow that produced an incorrect answer, determine which logic block is\n",
      "causally responsible for the failure.\n",
      ",→\n",
      ",→\n",
      "# Knowledge Base\n",
      "## Logic block types\n",
      "{logic_block_descriptions_text}\n",
      "## Operator types\n",
      "{operator_descriptions_text}\n",
      "# Responsibility Principles:\n",
      "- Consider blocks that **actually make mistakes** over blocks that only perform\n",
      "**redundant** work.,→\n",
      "- Our goal is to identify the weakest block in this workflow, so that in later\n",
      "optimization we can focus on improving this weakest block.,→\n",
      "- You will be given: the **problem**, the **correct answer**, the **incorrect\n",
      "answer**, the **workflow execution trace**, and **each block’s inputs/outputs**\n",
      "in a **sequential** pipeline. Ground your judgment in this evidence:\n",
      ",→\n",
      ",→\n",
      "- For each block *i*, compare its **output vs. input**, and **output vs. the\n",
      "correct answer** to locate where the **first critical deviation** was\n",
      "introduced, how later blocks **propagated/amplified** it, and whether any\n",
      "block **had enough information to correct** it but failed to do so.\n",
      ",→\n",
      ",→\n",
      ",→\n",
      "- Do **not** overweight temporal order:\n",
      "17\n",
      "- **Earlier** blocks bear more responsibility for **introducing** the critical\n",
      "error.,→\n",
      "- **Later** blocks bear responsibility for **failing to correct** earlier errors\n",
      "**given the available context**.,→\n",
      "- If two blocks seem equally responsible, apply **counterfactual** reasoning: *If\n",
      "this block were correct, would the final answer be correct?*,→\n",
      "- You may form a brief **internal** natural-language reason (e.g., “this block\n",
      "generated incorrect code”) to aid the decision, but the **output must be JSON\n",
      "only**.\n",
      ",→\n",
      ",→\n",
      "D OPTIMIZATIONPROMPT\n",
      "System Prompt\n",
      "You are an expert workflow optimization assistant specializing in Logic Block-based\n",
      "AI workflows for the {{dataset}} dataset.,→\n",
      "IMPORTANT: Focus exclusively on optimizing the low-performing logic block to improve\n",
      "code generation quality and overall workflow performance.,→\n",
      "IMPORTANT: You have exactly one optimization attempt. Reason carefully and aim to\n",
      "improve performance across the entire dataset.,→\n",
      "# Task Overview\n",
      "You will be provided with:\n",
      "1. Error examples showing: problem, correct answer, workflow's wrong answer, and the\n",
      "low-performing block's output,→\n",
      "2. Current workflow definition\n",
      "3. Performance analysis results\n",
      "Your objective: Optimize the identified low-performing logic block using the error\n",
      "examples as guidance while avoiding overfitting.,→\n",
      "# Logic Block Types and Detailed Semantics\n",
      "{logic_blocks_section}\n",
      "# Available Operators\n",
      "{operators_section}\n",
      "# Critical Instructions for Operator Usage\n",
      "**INSTRUCTION Field is Crucial**:\n",
      "- The`instruction`field is extremely important for operator performance and\n",
      "directly impacts final output quality,→\n",
      "- Instructions should clearly guide the operator on how to process input and produce\n",
      "expected output,→\n",
      "- For code generation tasks, instructions need to include specific programming\n",
      "requirements, output format, and quality standards,→\n",
      "- For mathematical reasoning tasks, instructions need to include specific\n",
      "problem-solving approaches, step-by-step reasoning requirements, and output\n",
      "format standards\n",
      ",→\n",
      ",→\n",
      "# Optimization Strategies\n",
      "Choose exactly one strategy:\n",
      "## 1. Add Block Strategy\n",
      "- Create a completely new logic block with its own name (e.g., \"b2\", \"b3\")\n",
      "- Insert the new block immediately before or after the low-performing block\n",
      "- Select appropriate block type (seq/for/cond) that complements the low-performing\n",
      "block,→\n",
      "18\n",
      "- Populate all required parameters (instructions, iteration limits, condition\n",
      "fields, etc.),→\n",
      "- Run internal counterfactual reasoning but do not output explorations\n",
      "Example:`\"workflow\": [\"b1\", \"b2\"] (\"b2\" performs worst)→\"workflow\": [\"b1\", \"b2\",\n",
      "\"b3\"]`,→\n",
      "## 2. Remove Block Strategy\n",
      "- Completely delete the low-performing block when it adds noise or harms outcomes\n",
      "- Internally evaluate workflow behavior without that block\n",
      "- Update workflow sequence and remove unused operators\n",
      "Example:`\"workflow\": [\"b1\", \"b2\"] (\"b1\" performs worst)→\"workflow\": [\"b2\"]`\n",
      "## 3. Modify Block Strategy\n",
      "- Rework the existing low-performing block without introducing new blocks\n",
      "- Examine block's logic type, operator choices, and parameterization\n",
      "- Update operators, ordering, and configuration for stronger reasoning\n",
      "- Focus solely on refining the current block\n",
      "# Critical Constraints\n",
      "CRITICAL: Maximum 3 blocks per workflow - DO NOT EXCEED this limit\n",
      "CRITICAL: Create NEW BLOCK with different name when adding\n",
      "IMPORTANT: Focus on the low-performing block identified in the analysis\n",
      "IMPORTANT: Maintain compatibility with other blocks in the workflow\n",
      "IMPORTANT: Each block should have a clear, distinct purpose\n",
      "# Prohibited Actions\n",
      "- NEVER reproduce workflow configurations matching provided history\n",
      "- MUST NOT repeat, reuse, or recycle any optimization from Previous Optimization\n",
      "Analysis,→\n",
      "- All workflows in previous optimization analysis are explicitly banned\n",
      "- Run internal \"novelty check\" to confirm at least two structural differences from\n",
      "banned workflows,→\n",
      "# Output Requirements\n",
      "- Apply exactly one modification strategy (Add/Remove/Modify)\n",
      "- Focus only on the identified low-performing logic block\n",
      "- Output clean JSON without comments or explanations\n",
      "- Ensure JSON is fully parseable and syntactically correct\n",
      "- Avoid overfitting to provided error examples\n",
      "User Prompt\n",
      "## Dataset\n",
      "<dataset>{dataset}</dataset>\n",
      "## Current Workflow Performance\n",
      "Current workflow score: <score>{score}</score>\n",
      "Low-performing logic block identified:\n",
      "<low_performing_blocks>{low_performing_blocks}</low_performing_blocks>\n",
      "## Current Workflow Definition\n",
      "```json\n",
      "<previous_code>{previous_code}</previous_code>\n",
      "```\n",
      "## Error Analysis\n",
      "19\n",
      "Error examples show:\n",
      "- **Problem**: Original code generation task/question\n",
      "- **Correct Answer**: Expected output\n",
      "- **Workflow Wrong Answer**: Current workflow output\n",
      "- **Low-performing Block Output**: Problematic block's specific output\n",
      "## Previous Optimization History\n",
      "STRICTLY PROHIBITED: Do not repeat or reuse any optimization results below.\n",
      "<reflection_result>{reflection_result}</reflection_result>\n",
      "IMPORTANT: All workflows above and current definition are disallowed baselines.\n",
      "# Optimization Task\n",
      "Analyze the low-performing logic block and improve its output quality.\n",
      "## Core Optimization Objective\n",
      "**Your optimization purpose is to modify the weakest block:**\n",
      "- Deeply analyze why this weak block led to the final incorrect answer\n",
      "- Understand the block's role and impact within the entire workflow\n",
      "- Identify the specific failure patterns and root causes of this block\n",
      "- Your chosen action (Add/Modify/Remove) should be aimed at solving the current\n",
      "problems,→\n",
      "## Key Focus Areas\n",
      "- Low-performing block is your primary optimization target\n",
      "- Use error cases to understand failure patterns\n",
      "- Improve block's reasoning or processing capability\n",
      "- Evaluate block type appropriateness (seq/for/cond)\n",
      "- Assess operator suitability and configuration\n",
      "- **Pay special attention to the quality and detail of instruction fields**\n",
      "## Strategy Guidelines\n",
      "Current workflow has\n",
      "<workflow_block_count>{workflow_block_count}</workflow_block_count> block(s).,→\n",
      "## Error Examples\n",
      "Use these to understand failures, but avoid overfitting:\n",
      "<error_cases_section>{error_cases_section}</error_cases_section>\n",
      "# Final Instruction\n",
      "Generate the optimized JSON workflow definition:\n",
      "E USE OFLARGELANGUAGEMODELS\n",
      "LLMs played a crucial role in our paper, as we utilized them for workflow optimization. Outside\n",
      "of this usage, we have used LLMs as writing assistants for improving clarity, style, and grammar\n",
      "and as coding assistants. Notably, the core research contributions—among which the design of\n",
      "the framework and validation of results—were conceived and verified exclusively by the authors.\n",
      "All outputs from LLMs were critically assessed, refined, and integrated to ensure correctness and\n",
      "adherence to academic standards.\n",
      "20\n"
     ]
    }
   ],
   "source": [
    "pdf_text = read_pdf(file_path)\n",
    "print(pdf_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e740903a",
   "metadata": {},
   "source": [
    "# Document Structure Aware"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd9d5366",
   "metadata": {},
   "source": [
    "## 2. Paragraph-based"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b6d347",
   "metadata": {},
   "source": [
    "### LangChain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a268d938",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "homeorag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
